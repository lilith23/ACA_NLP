{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: word vector representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import operator\n",
    "import itertools\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you define a couple of tokenizers and use them on a toy sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_1 = 'The quick brown fox jumps over the lazy dog.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - delimiter tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_by_split(text):\n",
    "    \"\"\"Tokenizes a given string of text by splitting words by whitespace\"\"\"\n",
    "    tokens = text.split(' ')\n",
    "    # your code goes here\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_by_split(test_sentence_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "assert tokenize_by_split(test_sentence_1) == ['The', 'quick', 'brown', 'fox', \n",
    "                                              'jumps', 'over', 'the', 'lazy', 'dog.']\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punkt_and_tokenize_by_split(text):\n",
    "    \"\"\"Replaces punktuation from given string of text with whitespace, then\n",
    "    tokenizes it by splitting words by whitespace\"\"\"\n",
    "    punkt_symbols = string.punctuation\n",
    "\n",
    "    # your code goes here\n",
    "    sentence = ''.join(word for word in text if word not in punkt_symbols)\n",
    "    removed = tokenize_by_split(sentence)\n",
    "    return removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punkt_and_tokenize_by_split(test_sentence_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "assert remove_punkt_and_tokenize_by_split(test_sentence_1) == ['The', 'quick', 'brown', 'fox', \n",
    "                                                               'jumps', 'over', 'the', 'lazy', 'dog']\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - using regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_2 = \"This is a test that isn't so simple: 1.23\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_by_regex(text):\n",
    "    \"\"\"Tokenizes a given string of text by applying the 'tokenize' method \n",
    "    of the provided 'tokenizer' object\"\"\"\n",
    "    tokenizer = nltk.RegexpTokenizer('\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # your code goes here\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "assert tokenize_by_regex(test_sentence_2) == ['This', 'is', 'a', 'test', 'that', \n",
    "                                              'isn', 't', 'so', 'simple', '1', '23']\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - using an advanced tokenizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use spaCy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_by_punkt_model(text):\n",
    "    \"\"\"Tokenizes a given string of text by applying spaCy nlp\"\"\"\n",
    "    # your code goes here\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Max Hermann was my friend.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = nlp('Max Hermann was my friend.')\n",
    "txt.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "assert tokenize_by_punkt_model(test_sentence_2) == ['This', 'is', 'a', 'test', 'that', \n",
    "                                                    'is', \"n't\", 'so', 'simple', ':', '1.23']\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: n-grams and stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. First of all, let's get it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/igel/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown corpus contains texts from different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_sents_ = list(nltk.corpus.brown.sents(categories=['adventure', 'science_fiction']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5585\n"
     ]
    }
   ],
   "source": [
    "print(len(adv_sents_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences from each category can be accessed as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4637\n"
     ]
    }
   ],
   "source": [
    "adv_sents = list(nltk.corpus.brown.sents(categories='adventure'))\n",
    "print(len(adv_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Dan',\n",
       "  'Morgan',\n",
       "  'told',\n",
       "  'himself',\n",
       "  'he',\n",
       "  'would',\n",
       "  'forget',\n",
       "  'Ann',\n",
       "  'Turner',\n",
       "  '.'],\n",
       " ['He', 'was', 'well', 'rid', 'of', 'her', '.']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_sents[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what are the most frequent unigrams in the 'adventure' category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joins a list of lists of tokens into a one large string of text\n",
    "words_of_text = list(itertools.chain.from_iterable(adv_sents))\n",
    "adventure_text = ' '.join(list(itertools.chain.from_iterable(adv_sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dan',\n",
       " 'Morgan',\n",
       " 'told',\n",
       " 'himself',\n",
       " 'he',\n",
       " 'would',\n",
       " 'forget',\n",
       " 'Ann',\n",
       " 'Turner',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_of_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dan Morgan told himself he would forget Ann Turner . He was '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adventure_text[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses the tokenizer function we've just written to tokenize text\n",
    "adventure_tokens = tokenize_by_regex(adventure_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dan',\n",
       " 'Morgan',\n",
       " 'told',\n",
       " 'himself',\n",
       " 'he',\n",
       " 'would',\n",
       " 'forget',\n",
       " 'Ann',\n",
       " 'Turner',\n",
       " 'He']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adventure_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60004\n"
     ]
    }
   ],
   "source": [
    "# turns each token to lowercase (simple normalization technique)\n",
    "lowered_tokens = [token.lower() for token in adventure_tokens]\n",
    "print(len(lowered_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "[('the', 3781), ('and', 1710), ('a', 1445)]\n"
     ]
    }
   ],
   "source": [
    "# counts the number of occurances for each unigram\n",
    "word_counter = collections.Counter(lowered_tokens)  # dictionary key = word, value = occurance\n",
    "print(word_counter['dan'])\n",
    "print(word_counter.most_common(3) ) # list of sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = [word[0] for word in word_counter.most_common(25)]\n",
    "values = [word[1] for word in word_counter.most_common(25)]\n",
    "indexes = np.arange(len(labels))\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(indexes, values)\n",
    "plt.xticks(indexes, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that some of the most common words above are not very interesting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stop-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exersise you remove stopwords, find the most frequent bigrams, then display them on a barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/igel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the list of english stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "print(len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - filtering stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords from the list of 'lowered_tokens'\n",
    "# your code goes here\n",
    "stopword_filtered_tokens = [token for token in \n",
    "                            lowered_tokens if token not in stopwords ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - getting the bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn your filtered list of unigrams into a list of bigrams, joint by whitespace\n",
    "# to achieve that, use the function nltk.ngrams(your_tokens, 2)\n",
    "# your code goes here\n",
    "\n",
    "filtered_bigrams = [' '.join(ngram) for ngram in nltk.ngrams(stopword_filtered_tokens ,2) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dan morgan', 'morgan told', 'told would', 'would forget', 'forget ann']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ye = stopword_filtered_tokens[:6]\n",
    "a = [' '.join(ngram) for ngram in nltk.ngrams(ye, 2)]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - counting occurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# now count the occurances of bigrams using a new Counter instance\n",
    "# your code goes here\n",
    "bigram_counter = collections.Counter(filtered_bigrams)\n",
    "\n",
    "assert {'miss langford', 'mary jane', 'billy tilghman'}.issubset(set(map(operator.itemgetter(0), \n",
    "                                                                         bigram_counter.most_common(15))))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIEAAAJCCAYAAABXmtfhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xu4bVddH/zvjxwgCBiBnFLk0mMpxUbFIBFFLkalvEiqQLlJLQZRg1pEtNg3j32rkWqNUsQq4muAGEREQYgEgkAEQkK4JlxyISIUDgWKISACagGB0T/m2Dlr77PWvq599skZn8/z7GfPNfdca4415pxjjvWdY81drbUAAAAAcGy7yV4XAAAAAIDdJwQCAAAAGIAQCAAAAGAAQiAAAACAAQiBAAAAAAYgBAIAAAAYgBAIAAAAYABCIAAAAIABCIEAAAAABrDvSK7sxBNPbAcOHDiSqwQAAAA4pl1xxRWfbK3t32i5DUOgqrpzkj9IcvskLck5rbX/UVVnJfmxJNf3RX++tfaq9V7rwIEDufzyyzdaJQAAAACbVFUf3sxymxkJ9KUk/7G19s6qunWSK6rqov63Z7bW/vt2CwkAAADAkbFhCNRa+3iSj/fpz1XVtUnuuNsFAwAAAGB5tnRj6Ko6kOSeSd7WZz2pqq6sqnOr6jZLLhsAAAAAS7LpEKiqbpXkpUme0lr7bJLfTXLXJCdnGin0jAXPO6OqLq+qy6+//vp5iwAAAACwyzYVAlXVTTMFQC9srb0sSVpr17XWvtxa+0qS5yS597znttbOaa2d0lo7Zf/+DW9UDQAAAMAu2DAEqqpK8rwk17bWfmNm/h1mFnt4kquXXzwAAAAAlmEz/x3svkkel+Sqqnp3n/fzSR5bVSdn+rfxB5M8cVdKCAAAAMCObea/g70pSc3506uWXxwAAAAAdsOW/jsYAAAAADdOQiAAAACAAQiBAAAAAAYgBAIAAAAYgBAIAAAAYABCIAAAAIABCIEAAAAABiAEAgAAABiAEAgAAABgAEIgAAAAgAEIgQAAAAAGIAQCAAAAGIAQCAAAAGAAQiAAAACAAQiBAAAAAAYgBAIAAAAYwL69LsCN1YEzL9zrIuyag2efttdFAAAAAJbMSCAAAACAAQiBAAAAAAYgBAIAAAAYgBAIAAAAYABCIAAAAIABCIEAAAAABiAEAgAAABiAEAgAAABgAEIgAAAAgAEIgQAAAAAGIAQCAAAAGIAQCAAAAGAAQiAAAACAAQiBAAAAAAYgBAIAAAAYgBAIAAAAYABCIAAAAIABCIEAAAAABiAEAgAAABiAEAgAAABgAEIgAAAAgAEIgQAAAAAGIAQCAAAAGIAQCAAAAGAAQiAAAACAAQiBAAAAAAYgBAIAAAAYgBAIAAAAYABCIAAAAIABCIEAAAAABiAEAgAAABiAEAgAAABgAEIgAAAAgAEIgQAAAAAGIAQCAAAAGIAQCAAAAGAAQiAAAACAAQiBAAAAAAYgBAIAAAAYgBAIAAAAYABCIAAAAIABCIEAAAAABiAEAgAAABiAEAgAAABgAEIgAAAAgAEIgQAAAAAGIAQCAAAAGIAQCAAAAGAAQiAAAACAAQiBAAAAAAYgBAIAAAAYgBAIAAAAYABCIAAAAIABCIEAAAAABiAEAgAAABiAEAgAAABgAEIgAAAAgAEIgQAAAAAGIAQCAAAAGIAQCAAAAGAAQiAAAACAAQiBAAAAAAYgBAIAAAAYgBAIAAAAYABCIAAAAIABCIEAAAAABiAEAgAAABiAEAgAAABgAEIgAAAAgAEIgQAAAAAGIAQCAAAAGIAQCAAAAGAAQiAAAACAAQiBAAAAAAYgBAIAAAAYgBAIAAAAYABCIAAAAIABCIEAAAAABiAEAgAAABiAEAgAAABgAEIgAAAAgAEIgQAAAAAGsGEIVFV3rqo3VNV7q+qaqvrpPv+2VXVRVb2//77N7hcXAAAAgO3YzEigLyX5j621k5J8e5L/UFUnJTkzyetaa3dL8rr+GAAAAICj0IYhUGvt4621d/bpzyW5Nskdkzw0yfP7Ys9P8rDdKiQAAAAAO7OlewJV1YEk90zytiS3b619vP/pr5PcfsFzzqiqy6vq8uuvv34HRQUAAABguzYdAlXVrZK8NMlTWmufnf1ba60lafOe11o7p7V2SmvtlP379++osAAAAABsz6ZCoKq6aaYA6IWttZf12ddV1R363++Q5BO7U0QAAAAAdmoz/x2skjwvybWttd+Y+dMFSU7v06cnefnyiwcAAADAMuzbxDL3TfK4JFdV1bv7vJ9PcnaSF1fVjyT5cJJH704RAQAAANipDUOg1tqbktSCP3/PcosDAAAAwG7Y0n8HAwAAAODGSQgEAAAAMAAhEAAAAMAAhEAAAAAAAxACAQAAAAxACAQAAAAwACEQAAAAwACEQAAAAAADEAIBAAAADEAIBAAAADAAIRAAAADAAIRAAAAAAAMQAgEAAAAMQAgEAAAAMAAhEAAAAMAAhEAAAAAAAxACAQAAAAxACAQAAAAwACEQAAAAwACEQAAAAAADEAIBAAAADEAIBAAAADAAIRAAAADAAIRAAAAAAAMQAgEAAAAMQAgEAAAAMAAhEAAAAMAAhEAAAAAAAxACAQAAAAxACAQAAAAwACEQAAAAwACEQAAAAAADEAIBAAAADEAIBAAAADAAIRAAAADAAIRAAAAAAAMQAgEAAAAMQAgEAAAAMIB9e10Ajg0Hzrxwr4uwaw6efdpeFwEAAAB2zEggAAAAgAEIgQAAAAAGIAQCAAAAGIAQCAAAAGAAQiAAAACAAQiBAAAAAAYgBAIAAAAYgBAIAAAAYABCIAAAAIABCIEAAAAABiAEAgAAABiAEAgAAABgAEIgAAAAgAEIgQAAAAAGIAQCAAAAGIAQCAAAAGAAQiAAAACAAQiBAAAAAAYgBAIAAAAYgBAIAAAAYABCIAAAAIABCIEAAAAABiAEAgAAABiAEAgAAABgAEIgAAAAgAEIgQAAAAAGIAQCAAAAGIAQCAAAAGAAQiAAAACAAQiBAAAAAAYgBAIAAAAYgBAIAAAAYABCIAAAAIABCIEAAAAABiAEAgAAABiAEAgAAABgAEIgAAAAgAEIgQAAAAAGIAQCAAAAGIAQCAAAAGAAQiAAAACAAQiBAAAAAAawb68LAMeqA2deuNdF2DUHzz5tr4sAAADAFhkJBAAAADAAIRAAAADAAIRAAAAAAAMQAgEAAAAMQAgEAAAAMAAhEAAAAMAAhEAAAAAAAxACAQAAAAxACAQAAAAwACEQAAAAwACEQAAAAAADEAIBAAAADEAIBAAAADAAIRAAAADAAIRAAAAAAAMQAgEAAAAMQAgEAAAAMIANQ6CqOreqPlFVV8/MO6uqPlZV7+4/D9ndYgIAAACwE5sZCXRekgfPmf/M1trJ/edVyy0WAAAAAMu0YQjUWrskyd8cgbIAAAAAsEt2ck+gJ1XVlf3rYrdZtFBVnVFVl1fV5ddff/0OVgcAAADAdm03BPrdJHdNcnKSjyd5xqIFW2vntNZOaa2dsn///m2uDgAAAICd2FYI1Fq7rrX25dbaV5I8J8m9l1ssAAAAAJZpWyFQVd1h5uHDk1y9aFkAAAAA9t6+jRaoqhclOTXJiVX10SS/mOTUqjo5SUtyMMkTd7GMAAAAAOzQhiFQa+2xc2Y/bxfKAgAAAMAu2cl/BwMAAADgRkIIBAAAADAAIRAAAADAAIRAAAAAAAMQAgEAAAAMQAgEAAAAMAAhEAAAAMAAhEAAAAAAAxACAQAAAAxACAQAAAAwACEQAAAAwACEQAAAAAADEAIBAAAADEAIBAAAADAAIRAAAADAAIRAAAAAAAPYt9cFAMZw4MwL97oIu+bg2aftdREAAAA2ZCQQAAAAwACEQAAAAAADEAIBAAAADEAIBAAAADAAIRAAAADAAIRAAAAAAAMQAgEAAAAMQAgEAAAAMAAhEAAAAMAAhEAAAAAAAxACAQAAAAxACAQAAAAwACEQAAAAwACEQAAAAAADEAIBAAAADEAIBAAAADAAIRAAAADAAIRAAAAAAAMQAgEAAAAMQAgEAAAAMIB9e10AgFEdOPPCvS7Crjl49ml7XQQAAGANI4EAAAAABiAEAgAAABiAEAgAAABgAEIgAAAAgAEIgQAAAAAGIAQCAAAAGIAQCAAAAGAAQiAAAACAAQiBAAAAAAYgBAIAAAAYgBAIAAAAYABCIAAAAIABCIEAAAAABiAEAgAAABiAEAgAAABgAEIgAAAAgAEIgQAAAAAGIAQCAAAAGIAQCAAAAGAAQiAAAACAAQiBAAAAAAawb68LAABJcuDMC/e6CLvm4Nmn7XURAADASCAAAACAEQiBAAAAAAYgBAIAAAAYgBAIAAAAYABCIAAAAIABCIEAAAAABiAEAgAAABiAEAgAAABgAEIgAAAAgAEIgQAAAAAGIAQCAAAAGIAQCAAAAGAAQiAAAACAAQiBAAAAAAYgBAIAAAAYgBAIAAAAYABCIAAAAIABCIEAAAAABiAEAgAAABiAEAgAAABgAEIgAAAAgAHs2+sCAADzHTjzwr0uwq45ePZpe10EAIDhGAkEAAAAMAAhEAAAAMAAhEAAAAAAAxACAQAAAAxACAQAAAAwACEQAAAAwACEQAAAAAADEAIBAAAADEAIBAAAADAAIRAAAADAAIRAAAAAAAMQAgEAAAAMQAgEAAAAMAAhEAAAAMAAhEAAAAAAAxACAQAAAAxACAQAAAAwgA1DoKo6t6o+UVVXz8y7bVVdVFXv779vs7vFBAAAAGAnNjMS6LwkD14z78wkr2ut3S3J6/pjAAAAAI5SG4ZArbVLkvzNmtkPTfL8Pv38JA9bcrkAAAAAWKJ923ze7VtrH+/Tf53k9osWrKozkpyRJHe5y122uToAgOTAmRfudRF2xcGzT9vrIgAAA9jxjaFbay1JW+fv57TWTmmtnbJ///6drg4AAACAbdhuCHRdVd0hSfrvTyyvSAAAAAAs23ZDoAuSnN6nT0/y8uUUBwAAAIDdsJl/Ef+iJG9Jcveq+mhV/UiSs5P866p6f5IH9scAAAAAHKU2vDF0a+2xC/70PUsuCwAAAAC7ZMc3hgYAAADg6CcEAgAAABiAEAgAAABgAEIgAAAAgAEIgQAAAAAGIAQCAAAAGIAQCAAAAGAAQiAAAACAAQiBAAAAAAYgBAIAAAAYgBAIAAAAYABCIAAAAIABCIEAAAAABiAEAgAAABiAEAgAAABgAEIgAAAAgAHs2+sCAACwPQfOvHCvi7BrDp592raed6zWifo43HbrBGBkRgIBAAAADEAIBAAAADAAIRAAAADAAIRAAAAAAAMQAgEAAAAMQAgEAAAAMAAhEAAAAMAAhEAAAAAAAxACAQAAAAxACAQAAAAwACEQAAAAwACEQAAAAAADEAIBAAAADEAIBAAAADAAIRAAAADAAIRAAAAAAAMQAgEAAAAMQAgEAAAAMAAhEAAAAMAAhEAAAAAAA9i31wUAAAA4Ug6ceeFeF2HXHDz7tC0/R33AWIwEAgAAABiAEAgAAABgAEIgAAAAgAEIgQAAAAAGIAQCAAAAGIAQCAAAAGAAQiAAAACAAQiBAAAAAAYgBAIAAAAYgBAIAAAAYABCIAAAAIABCIEAAAAABiAEAgAAABiAEAgAAABgAEIgAAAAgAEIgQAAAAAGIAQCAAAAGIAQCAAAAGAAQiAAAACAAQiBAAAAAAYgBAIAAAAYwL69LgAAAAAcLQ6ceeFeF2HXHDz7tC0/R30cW4wEAgAAABiAEAgAAABgAEIgAAAAgAEIgQAAAAAGIAQCAAAAGIAQCAAAAGAAQiAAAACAAQiBAAAAAAYgBAIAAAAYgBAIAAAAYABCIAAAAIABCIEAAAAABiAEAgAAABiAEAgAAABgAEIgAAAAgAEIgQAAAAAGIAQCAAAAGIAQCAAAAGAAQiAAAACAAQiBAAAAAAYgBAIAAAAYgBAIAAAAYABCIAAAAIABCIEAAAAABiAEAgAAABiAEAgAAABgAEIgAAAAgAEIgQAAAAAGIAQCAAAAGIAQCAAAAGAAQiAAAACAAQiBAAAAAAYgBAIAAAAYgBAIAAAAYABCIAAAAIABCIEAAAAABiAEAgAAABiAEAgAAABgAPt28uSqOpjkc0m+nORLrbVTllEoAAAAAJZrRyFQ912ttU8u4XUAAAAA2CW+DgYAAAAwgJ2GQC3Ja6vqiqo6YxkFAgAAAGD5dvp1sPu11j5WVf8kyUVV9ZettUtmF+jh0BlJcpe73GWHqwMAAABgO3Y0Eqi19rH++xNJzk9y7znLnNNaO6W1dsr+/ft3sjoAAAAAtmnbIVBV3bKqbr0yneRBSa5eVsEAAAAAWJ6dfB3s9knOr6qV1/mj1tqrl1IqAAAAAJZq2yFQa+2DSb55iWUBAAAAYJf4F/EAAAAAAxACAQAAAAxACAQAAAAwACEQAAAAwACEQAAAAAADEAIBAAAADEAIBAAAADAAIRAAAADAAIRAAAAAAAMQAgEAAAAMQAgEAAAAMAAhEAAAAMAAhEAAAAAAAxACAQAAAAxACAQAAAAwACEQAAAAwACEQAAAAAADEAIBAAAADEAIBAAAADAAIRAAAADAAIRAAAAAAAMQAgEAAAAMQAgEAAAAMAAhEAAAAMAAhEAAAAAAAxACAQAAAAxACAQAAAAwACEQAAAAwACEQAAAAAADEAIBAAAADEAIBAAAADAAIRAAAADAAIRAAAAAAAMQAgEAAAAMQAgEAAAAMAAhEAAAAMAAhEAAAAAAAxACAQAAAAxACAQAAAAwACEQAAAAwACEQAAAAAADEAIBAAAADEAIBAAAADAAIRAAAADAAIRAAAAAAAMQAgEAAAAMQAgEAAAAMAAhEAAAAMAAhEAAAAAAAxACAQAAAAxACAQAAAAwACEQAAAAwACEQAAAAAADEAIBAAAADEAIBAAAADAAIRAAAADAAIRAAAAAAAMQAgEAAAAMQAgEAAAAMAAhEAAAAMAAhEAAAAAAAxACAQAAAAxACAQAAAAwACEQAAAAwACEQAAAAAADEAIBAAAADEAIBAAAADAAIRAAAADAAIRAAAAAAAMQAgEAAAAMQAgEAAAAMAAhEAAAAMAAhEAAAAAAAxACAQAAAAxACAQAAAAwACEQAAAAwACEQAAAAAADEAIBAAAADEAIBAAAADAAIRAAAADAAIRAAAAAAAMQAgEAAAAMQAgEAAAAMAAhEAAAAMAAhEAAAAAAAxACAQAAAAxACAQAAAAwACEQAAAAwACEQAAAAAADEAIBAAAADEAIBAAAADAAIRAAAADAAIRAAAAAAAMQAgEAAAAMQAgEAAAAMAAhEAAAAMAAhEAAAAAAAxACAQAAAAxACAQAAAAwgB2FQFX14Kp6X1V9oKrOXFahAAAAAFiubYdAVXVckt9J8r1JTkry2Ko6aVkFAwAAAGB5djIS6N5JPtBa+2Br7YtJ/jjJQ5dTLAAAAACWaSch0B2TfGTm8Uf7PAAAAACOMtVa294Tqx6Z5MGttR/tjx+X5Ntaa09as9wZSc7oD++e5H3bL+6wTkzyyb0uxFFGnaymPg6nTlZTH4dTJ6upj8Opk8Opk9XUx+HUyWrq43DqZDX1cTh1sj3/rLW2f6OF9u1gBR9LcueZx3fq81ZprZ2T5JwdrGd4VXV5a+2UvS7H0USdrKY+DqdOVlMfh1Mnq6mPw6mTw6mT1dTH4dTJaurjcOpkNfVxOHWyu3bydbB3JLlbVX1dVd0syQ8kuWA5xQIAAABgmbY9Eqi19qWqelKS1yQ5Lsm5rbVrllYyAAAAAJZmJ18HS2vtVUletaSysJiv0x1OnaymPg6nTlZTH4dTJ6upj8Opk8Opk9XUx+HUyWrq43DqZDX1cTh1sou2fWNoAAAAAG48dnJPIAAAAABuJIRAR5GqOlBVVy/428VVdaO5Q3pVfX9VnbnD1zhYVScuq0wzr/v0qrqmqp6+g9fYsGxHYntW1clV9ZCZx2dV1VN3+roL1vWUqvqqXXjdr6mqn1z26x7LqurvNvi7Ot2GqvraqvrTvS7HRpbVNm6mvaiq86rqkTtd15zXfXxVPWsXXne9dve5VXXSstfJ0W+jNnMJr7/2XLzjPtA2yrCpfX+2/dhKvfTX/3czj0+pqt/q0xsez1V1alW9crPruzHZTn9vt/q4u2GZfcsb0/tmfTtt55b1uWIL7c937HRdxxIhELuitXZBa+3svS7HAmckuUdr7ec2s3BV7ejeWbvs5CQP2XCpGTXZzrH/lCRLD4GSfE0SgcVyqdMZmz2GW2v/u7W29MCDo0Nr7Udba+/d63LcGFXVcXtdhqPcqnPx0dYHWtK+fyDJDSFQa+3y1tqTd/iaLNFR3l8dwkjbYAnt3G59rpjn1CRbCoGO9W0pBFqSqvqhqrqyqt5TVS/o8w5U1ev7/NdV1V36/FVXVuddiamqW1TVH1fVtVV1fpJbLFjv2VX13r6O/97n7a+ql1bVO/rPffv8W1bVuVX19qp6V1U9dBvv80BV/WV/D39VVS+sqgdW1WVV9f6qundf7oZUtqoeVVVX97q5pM/7hl6Od/ey322D9f5ZVV1R0wieM2brrqp+pb/2W6vq9n3+Xfvjq6rql1fquKouSHKrJFdU1WM22Eb/f1W9LcmvV9Xtquq1ff3PTVKbrLJ9vY6urao/nZd4z27/qnpkVZ3Xp+dux5llb5bkaUke0+vxMf1PJ/UrTx+sqifPbLf3VdUfJLk6yZ2r6rG9fq6uql+bed3frarL+3v9pT7vyUm+NskbquoNm3zvm3V2krv29/D0qvqdqvr+vt7zq+rcPv2EqvqVPv2zvdxXV9VT5r1o3zdWRn39RVXde6ZeVl7/uL7MO/o+8MQ+/9S+7J/2/f2FVbXZbX5EVdXPzZT/l/rsVXU65zn/pe8Pb6qqF1W/wlfT1ey39tc6v6pucyTfy5oybratuXdVvaWmNu3NVXX3Pv/xVXVBVb0+yeuq6g+q6mEzr//CWtMG1szV3D59aVW9s/98R5+/cN+oqntV1RtraqteU1V32GEd3LKqLqypfbt65hhPkp/q5bqqqr6+L3/bmtrKK/t2vMd689es68eq6s+rat655gG9bj9Yq89d8/a99drrH+7b8u1J7pvdM7fdrZkr8lX1Iytlqarn1JyriDVd/X5+3w8+XFX/tqp+vdf5q6vqpn25X+j1cHVVnTOzP1xcVb/W1/FXVXX/XXzPK2V+Ws20iTWdH3+6T291e/1dVT2jqt6T5D41p7+xZt2LjsWvqqoX9+eeX1Vvm9kOc89Du6UmT+/ru2rlmNrguH5In3dFVf1WrRnNUnPOxbW6D3ReTefVt/Zj6NSa+mLXVj/f9+Ue1OvvnVX1kqq61Tbe4ob7/oJ62bB9zHReuX9/jz9TC0b21IL+V3erBXV8sKp+tb/25VX1LTW1of+zqn68L3OrmvppK+3eQ/v8A/39Pqfvw6+t+e3Yove+J/33bl47PrevXuufky6tqX+7tJC7qv5zTe3Wm5LcfWb+XWtq/67o610p9/f1Y/tdNfW5Vvrjm+o/1+b6bFs+Ly9TLe47XVxVv1lVlyf56Xl1UVU3qanvsr8/5yZV9YGVxzPrWLT9L6mqk2eWe1NVffM6y2/pc9aaMmzns955NbWPq/oKtaadqKpn9ecd9rmiFrSBtcG5Z03Z59X9gSQ/nuRnen3cvxZ/Vj6rql5QVZclecFm6+xGqbXmZ4c/Sb4hyV8lObE/vm3//Yokp/fpJyT5sz59XpJHzjz/7/rvA0mu7tM/m+TcPn2PJF9Kcsqa9d4uyfuSG27w/TX99x8luV+fvkuSa/v0f0vy71eW7WW+5Rbf64Felm/KFCJekeTcTI36Q2fe4+OTPKtPX5XkjmvK+NtJfrBP3yzJLeas6+CcOr1FphDjdv1xS/J9ffrXk/x/ffqVSR7bp398pY5n63sT2+iVSY7rj38ryS/06dP6ek/cRF21JPftj89N8tQ+ffHK9lxTnkcmOW+97bhmHTfUc398VpI3J7l5khOTfCrJTXtZvpLk2/tyX5vkfyXZn+m/BL4+ycPW1PVxvZz3WLs9lnz8HEjf7/vjH0jy9D799iRv7dO/n+T/SXKvvk/dMlOgd02Se8553Zbke/v0+Ule2+vim5O8u88/Y2afuXmSy5N8XaYrBp9JcqdM+/lbVrbF0fCTQ23GgzL994Tq5XxlkgesrdM1z/3WJO9OcnySWyd5/8x+eWWS7+zTT0vym3v4Hg9kc23NVyfZ16cfmOSlM8fGR2f25++cec4JST608rx5+2Kmq1PH9+m7Jbm8T8/dN/q+9eYk+/tyj0lvw3dQB49I8pyZxyf03weT/FSf/skkz+3Tv53kF/v0d8/s54vmn5XkqUmelOTlSW4+pwznJXlJf68nJfnAevte/9th7XWSO+RQm3OzJJdlpu1a8n6zbrubqf07mOS2fbtdOq8svX7elEPtxj9kdZuyqs3s0y/IoXPSxUme0acfkuQvjtBx884+fZMk/7PX/5a2V3/ckjy6T8/tb6xZ96Jj8alJfq9Pf2N6fybrnId2oV5W2sxHJLko0/nt9n39d8ji4/r4JB9J8nX9+S9K8so5r//4rD4X3/A40zH0xznUdn02q9u1kzOdry9J75Ml+X/T+xzL3PfbofbjxDX1spn28dTZ9z77eM37ndv/WlTHM2X6iT79zEznolv3feO6Pn9fkq/u0ycm+UCv0wN9nzq5/+3F6X3dTdTZnvTfZ97zvHZ8bl8965+T/j59H13S8bLSz/qqTMf1B2b2pdcluVuf/rYkr+/Tt8mh9uFHc6jt21T/OZvrs23pvLzkNmS9vtPFSZ49s+yiuvjFJE/p0w9KbyPXrGfR9j89vU+W5F/OvPdFy2/4OWuDtmSrn/XOy/y+wqlZ3W48K8nj57RFc9vAbO7cM1uORXV/1sr26o8XfVY+q7/fTdfXjfXnmB7mdAR9d5KXtNY+mSSttb/p8++T5N/26RdkCik26wGZGs601q6sqivnLPOZJJ9P8ryesq4krQ/MNBpkZbmv7mnqg5J8fx36Xu/x6Tv+FsqVJB9qrV2VJFX+PgccAAAMU0lEQVR1TZLXtdZaVV2VqeFY67Ik51XVi5O8rM97S5L/XFV3SvKy1tr7N1jnk6vq4X36zpka/08l+WIOve8rkvzrPn2fJCtXtf4oyaLkeL1t9JLW2pf79ANWlmutXVhVn96gvCs+0lq7rE//YZInr1OWteZux9baRt/hv7C19oUkX6iqT2Tq6CbJh1trb+3T35rk4tba9cl01S/Te/yzJI+u6Wrwvkyd45MydciOlEuTPKWm+xe8N8ltahpRcZ9M9feEJOe31v6+l/1lSe6f5F1rXueLSV7dp69K8oXW2j+u2U8flOQedejK3gmZ9q0vJnl7a+2jfR3v7s9503Lf6o49qP+svPdbZSr//1rnOfdN8vLW2ueTfL6qXpEkVXVCppPrG/tyz890Qt9Lm2lrTkjy/H6Vq2XqNK64aKU9bq29saqe3a+6PSJT5+tL66z7pkme1a+8fTlTp2vFvH3jbzN9wL2oH7PHJfn4tt/55Kokz6hphMQrW2uXzvxtpS29IofasPtlem9prb2+piuwX73O/CT5oUwfch/WWvvHBeX4s9baV5K8t/rV3Sze9y7J/Pb6n2Z1m/MnWV2ny7RRu3vvJG9c2Teq6iXrlOXPZ9qN47K6TTnQp7+rqv5Tpg8ot80UTL+i/212O60sv2taawer6lNVdc9Mbf+7Wmufqqqtbq9PZdrvX9rnL+pvzFp0LN4vyf/o5bt6pj+z3nlot9wvyYv6uf26qnpjL8dnM/+4/rskH2ytfag//0WZLh5s1Stm2q7r1rRrBzJ9eD0pyWW9/bhZpn7SVm2rz7GN9nE96/W/1juvXtB/X5XkVq21zyX5XFV9oaq+JlPQ8d+q6gGZLmrdMYf6Nx9qrb27T2/lWNur/vuKee34or76/87656QPZXnun6mf9Q9JUtMoo/TPEt+R5CUzfdOb9993SvInvb92s0xBYrL5/vNm+mxbPS8vs882t+80409mphfVxbmZLrj8Zqa+7O/PWc+i7f+SJP+lqn6uP/e8DZbf6uestbb6WS+Z31fYrG/P/DZwM+eeWYvqfq1Fn5WT5ILW2v/ZYvlvdIRAe+NL6V/Fq+neLDfbzou01r5U05C878k0guRJmU5oN8k04uPzs8vXtKc/orX2vh2UPUm+MDP9lZnHX8mcfaq19uNV9W2ZrgBcUVX3aq39UU1ftTotyauq6omttdfPW1lVnZrpYL1Pa+0fquriTI1ckvxja1N0m+mEsMx9+u+X8Bptg8dr5x0/Mz13O27C7PaZrZMN309VfV2mq7bf2lr7dE1D1Y9f/1nL1Vr7WO/wPTjTB5TbJnl0pitun6vNj/Cd3Tdu2E9ba1+pQ9/zrUxX4l4z+8S+zy2qx6NJJfnV1trvrZo5DX09FmymrfmvSd7QWnt4f98Xzzxn7T7/B0n+fabRZj+8wbp/Jsl1ma5C3iRTJ2ReuVb2jUpyTWvtPhu87qa11v6qqr4l0yiSX66q17XWnramDDvdN6/KNBLhTlncWZp9vzXze96+d2oWt9dHymba3c2abTfWtin7qur4JM/OdKX/I1V1Vla/32Vtp614bqYro/8004eOZHvb6/MrF0LW6W/MWu9YvDHYzTZ/tu1a267t6+u7qLX22B2uZyf7/lbax+1ar443qqMfzDQy6F49HDiYQ/vq2tfd9NfBtmgp/fcZ89qHuX313rYsOicto7+6GTdJ8rettZPn/O23k/xGa+2C3q6ctcXX3kyfbavn5SNpdhvMrYt+jriuqr4708WIH5zzOgs/q1XVRZlG4jw602it9Za/drOfsxbY0me9Oc9Z6SvccMx0i/oDlQVt4CbOPbM2ux8u+qycHLnjaU+5J9ByvD7Jo6rqdsl0/4U+/82ZTqbJdKCvXMU9mEMH7/dn9ZXrFZek34Cvqr4x05DSVXpieUJr7VWZGsZv7n96bZKfmllupbF+TabvH698B/ueW3mT21VVd22tva219gtJrs90P5p/nunq2m9lSsUPe38zTkjy6d5B/fpMafFG3pp+5TuHtsE8i7bRWrPb43szDTfcjLtU1cqHwn+X+Vclrquqf9U7FA+fmb9oO876XKZhqVv19iTfWVUn1nTDz8cmeWOmYb9/n+QzPcX/3iWsayPzXvetmW4Yd0mmbfLUHNo2lyZ5WE33mbhlpjpbtN028pokP1GH7u3xL/tr3li8JskT6tD3pu9YVf8k62+ry5J8X1Ud35/3b5KktfaZJJ+uQ/cteVymfeJod0KSj/Xpx2+w7HmZ9qu0jW+SekKSj/erWo/LNApkPe9Lsn/leK+qm1bVN2zwnHVV1dcm+YfW2h8meXqSb9ngKZemdyp75+eTrbXPrjM/mUaGPDHJBX19m7Vo31vUXr8tU5tzu368PWoL69qqjdrdd/Sy3KZ/uHhEtm+lQ/vJXhdHw43Fz88Uon9rpu2UbH17rbJOf2PWomPxskwfWlLTCM9v6vMXnYd206WZ7t1zXE2jXh7Qy7HI+5L885lg/TELltvp+fGtSe5bVf8iueG+INsZKbeZPsci52X99nGz73Gz/a+tOiHJJ3oA9F1J/tkSXnNP+u8bWNRX3+o5aScuydTPukVV3TrJ9yVJP298qKoe1ctWVbXSFswe/6evea3t9J/nOZJ1sNbcvtMCi+oimUL6P8zqbxvMWu+z2nMzjTJ7R2vt0+stv8XPWbvpw5lG3Ny8X+D9npm/zbYpc9vATZ57Zi2q+7Xt12Y+Yx3ThEBL0Fq7JsmvJHljTTdQ/I3+p59K8sM1DQV9XJKf7vOfk6nj855MQ07nJY6/m+kGetdmujfHFXOWuXWSV/bXf1Om7yEn0/DfU2q6gdZ7M30nO5mu0t00yZU1De37r9t9z1v09Oo3fsx0Yn1Ppg7h1TUN2fzGTFegFnl1piuu12a6MeFb11l2xVOS/Gyvm3+RaTjhPIu20Vq/lOnmqNdkGta63tdtZr0vyX/oZb9Npu261pmZhje+Oau/PrJoO856Q6bGdfbG0BtqrX28r/cNmbbHFa21l7fW3pPpQ+FfZhrGfdnM085J8upa8o2hW2ufyjT88+o6dBPjSzPdj+ADSd6ZaTTQpX35d2bqrL490wfL57bW1n4VbLOem+krZ+/s++fv5egc8TNXa+21mbbTW2oaovunSW69oE5XnvOOTMPur0zy55lGgqwcH6dnOl6vzDQ65Gk5+v16kl+tqndlg23XWrsu09df5w3BvmGx/vvZSU7v7fTXZ4MrQ621L2YKAH6tP+fd2eJ/opjjm5K8vbeTv5jklzdY/qwk9+rb7+wc6gAtmr9S9jdlClovrE3+695F+14WtNe9zTkr0/Duy7L1ryFvxbrtbmvtY5nuo/D2XpaDWXyOWFdr7W8zndOvztQZf8e2S70kfV98Q5IXz4zk2dL2mmNRf2PWomPx2ZkC0vdm2oevSfKZReeh7b/zTTk/U9v3nkwBwH9qrf31ooX7VwJ+MtO574pMHyTm7SvbOhfPrOf6TMHZi3odvyVTu7NVm+lzLCrDRu3jlUm+XNMNlH9mnZfabP9rq16YqU90Vaavsf7lTl9wD/vv61nUV9/SOWknej/rTzIdJ3+e1e3aDyb5kV6OazKNTEmm9v0l/Tj55Mzy2+0/z3PE6mCtDfpOa52V+XWR/hq3yuLjbOFntdbaFZm+uvr7m1h+K5+zdk1r7SOZ7tN1df8921+/4XPFOm3gZs49s87K/Lp/RZKH9zb6/tncZ6xj2sqNk+CYUtN/xPg//furP5DpJoVb/m9ocCyqfm+pfpxckuSM3uk7pvX3e1WSb+kjn9b+/V6ZhhF/5xEvHEfUzDGwL1MwcG5r7fy9Ltcy1DSq9J1JHrWN+0DsRnmOS3LT1trnq+quSf4iyd17WHXUm9lXKsnvJHl/a+2Ze12uZduofdzi6+h/ccxZRt+ppv/S98zW2pb/W2RNo3UvTvL1fTQUbNuN5oo3bNG9Mt08rjLdsPUJe1weOJqcU9PXMo5P8vxBAqAHJnleps7XvADolEwjJc480mVjT5zV94njMw0L382bER8x/bh+Zaabuu55ANR9VaZ/A3zTTPd9+MkbSwDU/VhVnZ7p/i/vyjRi9JiyUfu4RfpfHKt21HeqqjOT/ETm3wtoo+f+UKZRaz8rAGIZjAQCAAAAGIB7AgEAAAAMQAgEAAAAMAAhEAAAAMAAhEAAAAAAAxACAQAAAAxACAQAAAAwgP8LPd9/GeZ9gdkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = [w[0] for w in bigram_counter.most_common(15)]\n",
    "values = [w[1] for w in bigram_counter.most_common(15)]\n",
    "indexes = np.arange(len(labels))\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(indexes, values)\n",
    "plt.xticks(indexes, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home Exercise 1: vocabularies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you write a function that builds a vocabulary from the provided text corpus. Then you use it to encode tokens into numeric form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - building a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(tokens, max_size):\n",
    "    \"\"\"\n",
    "    Builds a dictionary of at most max_size most frequent tokens from the supplied list of tokens.\n",
    "    More frequent tokens should have a lower id, but that is not strictly required.\n",
    "    Two special symbols 'NULL':0 and 'UNKN':1 should also be added to the dictionary.\n",
    "    \n",
    "    EXAMPLE:\n",
    "    {\n",
    "        'NULL': 0,\n",
    "        'UNKN': 1,\n",
    "        'the': 2,\n",
    "        'and': 3,\n",
    "        'a': 4,\n",
    "        ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    vocabulary = {}\n",
    "    reserved_symbols = [\"NULL\", \"UNKN\"]\n",
    "    \n",
    "    # your code goes here\n",
    "\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(tokens, max_size=20000):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary of at most max_size words from the supplied list of lists of tokens.\n",
    "    If a word embedding model is provided, adds only the words present in the model vocabulary.\n",
    "    \"\"\"\n",
    "    # your code goes here\n",
    "    vocabulary = {}\n",
    "    reserved_symbols = [\"NULL\", \"UNKN\"]\n",
    "    \n",
    "    counter = collections.Counter(tokens)\n",
    "    \n",
    "    freq_toks = counter.most_common(max_size-len(reserved_symbols)) # list of sets (work, occur)\n",
    "\n",
    "    voc_words = [k[0] for k in freq_toks] # list of words\n",
    "\n",
    "    for i, reserved in enumerate(reserved_symbols):\n",
    "        vocabulary[reserved] = i\n",
    "\n",
    "    for i, k in enumerate(voc_words):\n",
    "        vocabulary[k] = i+len(reserved_symbols)\n",
    "\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = ['mek', 'erku', 4, 'mors']\n",
    "di = {}\n",
    "for i, num in enumerate(li):\n",
    "    di[num] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: 2, 'erku': 1, 'mek': 0, 'mors': 3}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "VOC_SIZE = 5000\n",
    "\n",
    "my_vocabulary = build_vocabulary(lowered_tokens, VOC_SIZE)\n",
    "\n",
    "assert len(my_vocabulary) == VOC_SIZE\n",
    "assert {'NULL', 'UNKN'}.issubset(set(my_vocabulary.keys()))\n",
    "assert set([w[0] for w in word_counter.most_common(VOC_SIZE-10)]).issubset(set(my_vocabulary.keys()))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_dictionary = {val: keey for keey, val in my_vocabulary.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - encoding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_tokens(sentence, tokenizer, token_to_id, max_len):\n",
    "    \"\"\"\n",
    "    Converts a list of tokens to a list of token ids using the supplied dictionary.\n",
    "    Pads resulting list with NULL identifiers up to max_len length.\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    \n",
    "    # STEP 1: convert sentence to a list of tokens\n",
    "    # your code goes here\n",
    "    tokens = tokenizer(sentence)\n",
    "    \n",
    "    # STEP 2: replace tokens with their identifiers from the vocabulary\n",
    "    # If the token is not present in the vocabulary, replace it with UNKN identifier\n",
    "    encoded_tokens = [token_to_id.get(token, token_to_id['UNKN']) \n",
    "     for token in tokens] \n",
    "    # token_to_id- n vocabularyn e, get-- ete chka, veradarcni UNKN-in hamapatasxan bary\n",
    "\n",
    "    # STEP 3: pad the sequence id's with NULL identifiers until so that it's length is equal to max_len\n",
    "      \n",
    "    return encoded_tokens[:max_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 16\n",
    "test_sentence = 'The animals thundered away into the moonlight , heading for the ridges .'\n",
    "vectorized = vectorize_tokens(test_sentence,\n",
    "                              tokenize_by_regex, my_vocabulary, MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1065, 1, 94, 43, 2, 1740, 1727, 22, 2, 4223]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-e66fe83a0133>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m assert [my_vocabulary.get(t, my_vocabulary['UNKN']) \n\u001b[1;32m      3\u001b[0m         for t in tokenize_by_regex(test_sentence)] + [0]*(MAX_LEN-len(tokenize_by_regex(test_sentence))) == vectorized\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert len(vectorized) == MAX_LEN\n",
    "assert [my_vocabulary.get(t, my_vocabulary['UNKN']) \n",
    "        for t in tokenize_by_regex(test_sentence)] + [0]*(MAX_LEN-len(tokenize_by_regex(test_sentence))) == vectorized\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you create a function to compute sentence similarity, then build a simple Information Retrieval system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Dan Morgan told himself he would forget Ann Turner .',\n",
       " 'He was well rid of her .',\n",
       " \"He certainly didn't want a wife who was fickle as Ann .\"]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOC_SIZE = 5000\n",
    "\n",
    "adv_brown_sents = [' '.join(sent) for sent in nltk.corpus.brown.sents(categories='adventure')]\n",
    "print(len(adv_brown_sents))\n",
    "adv_brown_sents[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dan', 'Morgan', 'told', 'himself', 'he', 'would', 'forget', 'Ann', 'Turner', '.']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.brown.sents(categories='adventure')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the CountVectorizer instance\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=VOC_SIZE, stop_words=stopwords, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words={\"didn't\", 'does', 'above', 'being', 'with', 'below', 'before', \"you'd\", 'yourself', 'same', 'this', 'and', 'over', 'again', 'myself', 'were', 'an', 'which', 'she', 'are', 'how', \"you've\", 'these', 'am', 'yours', 'should', 'we', 'but', \"wasn't\", 'themselves', \"that'll\", 're', 'by', 't', '...uldn't\", 'between', 'too', 'through', 'on', 'y', 'during', 'will', \"mustn't\", 'did', 'such', 'very'},\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# builds the vocabulary from the data\n",
    "tfidf_vectorizer.fit(adv_brown_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_vocab = dict(zip(range(len(tfidf_vectorizer.get_feature_names())),\n",
    "                                  tfidf_vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '10',\n",
       " '1895',\n",
       " '200',\n",
       " '21',\n",
       " '275',\n",
       " '30',\n",
       " '300',\n",
       " '600',\n",
       " 'aaron',\n",
       " 'able',\n",
       " 'abruptly',\n",
       " 'accepting',\n",
       " 'accident',\n",
       " 'accompanied',\n",
       " 'account',\n",
       " 'accused',\n",
       " 'accusing',\n",
       " 'accustomed',\n",
       " 'ace']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4637, 5000)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applies one-hot encoding to the provided data, transforming sentences into vectors\n",
    "vectorized_sents = tfidf_vectorizer.transform(adv_brown_sents)\n",
    "\n",
    "# the resulting matrix has shape (N_SAMPLES x VOC_SIZE)\n",
    "vectorized_sents.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the sentence vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dan Morgan told himself he would forget Ann Turner .\n"
     ]
    }
   ],
   "source": [
    "# sentence\n",
    "print(adv_brown_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# sentence vector is almost all zeroes\n",
    "print(vectorized_sents[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4932 4389 4251 2021 1219  751  102]\n"
     ]
    }
   ],
   "source": [
    "# nonzero elements of the sentence vector\n",
    "first_sent = vectorized_sents[0].nonzero()[1]\n",
    "print(first_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['would', 'turner', 'told', 'morgan', 'forget', 'dan', 'ann']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the words are the same, but the word order is lost, and stopwords are removed\n",
    "[tfidf_vectorizer_vocab[wid] for wid in first_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.20980297]]\n"
     ]
    }
   ],
   "source": [
    "# we can now compute the similarity between sentences like so:\n",
    "sent1vector = vectorized_sents[0]\n",
    "sent10vector = vectorized_sents[10]\n",
    "similarity = cosine_similarity(sent1vector, sent10vector)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_sentence_similarity(sent1, sent2, vectorizer):\n",
    "    \"\"\"Encodes provided sentences using the 'vectorizer' object,\n",
    "    then computes the cosine similarity between sentence vectors\n",
    "    Outputs a real number between [0,1] \"\"\"\n",
    "    \n",
    "    # CountVectorizer requires a list of sentences as input\n",
    "    sent1_ = [sent1]\n",
    "    sent2_ = [sent2]\n",
    "    \n",
    "    # your code goes here\n",
    "    \n",
    "    vectorized_sent1 = vectorizer.transform(sent1_)\n",
    "    vectorized_sent2 = vectorizer.transform(sent2_)\n",
    "    similarity = cosine_similarity(vectorized_sent1, vectorized_sent2)\n",
    "\n",
    "\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_1 = 'I like building robots'\n",
    "test_sentence_2 = 'I also like building pillow fortresses'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.81091811]]\n"
     ]
    }
   ],
   "source": [
    "print(onehot_sentence_similarity(test_sentence_1, test_sentence_2, tfidf_vectorizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - information retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEngine(object):\n",
    "    def __init__(self, knowledge_base, voc_size=5000):\n",
    "        \"\"\"\n",
    "        Implements a simple information retrieval system based on Tf-Idf text representation.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.kbase = np.array(knowledge_base)\n",
    "        self.vectorizer = TfidfVectorizer(max_features=voc_size)\n",
    "        self.vectorized_kbase = self.vectorizer.fit_transform(knowledge_base)\n",
    "        \n",
    "    def search(self, query, top_k=3):\n",
    "        \"\"\"\n",
    "        Retrieves the top-k documents from the knowledge_base most similar to given query\n",
    "        \"\"\"\n",
    "        \n",
    "        vectorized_query = self.vectorizer.transform([query])\n",
    "        \n",
    "        # your code goes here\n",
    "        # STEP 1: compute similarities between query and all documents in knowledge base\n",
    "\n",
    "        # STEP 2: sort the similarities to find most similar document indices\n",
    "        # HINT: use np.argsort to do that\n",
    "        \n",
    "        # STEP 3: gets top-k most similar documents from self.kbase, returns them\n",
    "        \n",
    "        return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEngine(object):\n",
    "    def __init__(self, knowledge_base, voc_size=5000):\n",
    "        \"\"\"\n",
    "        Implements a simple information retrieval system based on Tf-Idf text representation.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.kbase = np.array(knowledge_base)\n",
    "        self.vectorizer = TfidfVectorizer(max_features=voc_size)\n",
    "        self.vectorized_kbase = self.vectorizer.fit_transform(knowledge_base)\n",
    "        \n",
    "    def search(self, query, top_k=3):\n",
    "        \"\"\"\n",
    "        Retrieves the top-k documents from the knowledge_base most similar to given query\n",
    "        \"\"\"\n",
    "        \n",
    "        vectorized_query = self.vectorizer.transform([query])\n",
    "        \n",
    "        # your code goes here\n",
    "        # STEP 1: compute similarities between query and all documents in knowledge base\n",
    "        \n",
    "        sim = cosine_similarity(self.vectorized_kbase, vectorized_query)\n",
    "        sim.reshape(-1,1)\n",
    "        print(sim.shape)\n",
    "        \n",
    "        # STEP 2: sort the similarities to find most similar document indices\n",
    "        # HINT: use np.argsort to do that\n",
    "        indic =  np.argsort(sim)\n",
    "        print(indic)\n",
    "        # STEP 3: gets top-k most similar documents from self.kbase, returns them\n",
    "        top_indic = indic[ -top_k+1 :]\n",
    "        print(top_indic)\n",
    "        results   = [self.kbase[ind] for ind in top_indic]\n",
    "        print(results)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = SearchEngine(adv_brown_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4637, 1)\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[0]\n",
      " [0]]\n",
      "[array(['Dan Morgan told himself he would forget Ann Turner .'],\n",
      "      dtype='<U754'), array(['Dan Morgan told himself he would forget Ann Turner .'],\n",
      "      dtype='<U754')]\n",
      "(4637, 1)\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[0]]\n",
      "[array(['Dan Morgan told himself he would forget Ann Turner .'],\n",
      "      dtype='<U754')]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "query1 = 'take it easy'\n",
    "result1 = se.search(query1, top_k=3)[0]\n",
    "# assert query1 in result1\n",
    "\n",
    "query2 = 'uneasy feeling'\n",
    "result2 = se.search(query2, top_k=2)[0]\n",
    "# assert query2 in result2\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4637\n"
     ]
    }
   ],
   "source": [
    "print(len(result1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home Exercise 1: language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you build a 3-gram language model, then use it to generate grammaticaly valid text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 57340/57340 [00:06<00:00, 9295.04it/s]\n"
     ]
    }
   ],
   "source": [
    "model = collections.defaultdict(lambda: collections.defaultdict(lambda: 0))\n",
    " \n",
    "for sentence in tqdm(nltk.corpus.brown.sents()):\n",
    "    for w1, w2, w3 in nltk.trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        # iterate over all trigrams, accumulate co-occurance counts\n",
    "        model[(w1, w2)][w3] += 1\n",
    "\n",
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        # normalize counts to produce a valid probability distribution\n",
    "        model[w1_w2][w3] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One ground of the amended Act provides that final Sunday at First Christian Church of England , which he may indeed have I realized how limited was his own picture books are enough for a time he moved up beside them .\n"
     ]
    }
   ],
   "source": [
    "text = [None, None]\n",
    " \n",
    "sentence_finished = False\n",
    " \n",
    "while not sentence_finished:\n",
    "    # introduce a stochastic variable\n",
    "    r = random.random()\n",
    "    accumulator = .0\n",
    "    current_bigram = tuple(text[-2:])\n",
    "\n",
    "    for word, pr in model[current_bigram].items():\n",
    "        accumulator += pr\n",
    "        \n",
    "        # frequent trigrams are more likely to overflow accumulator:\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    " \n",
    "    if text[-2:] == [None, None]:\n",
    "        sentence_finished = True\n",
    "\n",
    "print(' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 By analogy, implement a 4-gram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
