{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec: skip gram & cbow\n",
    "\n",
    "Models __CBOW (Continuous Bag of Words)__ and __Skip gram__ were invented in the now distant 2013,\n",
    "*article*:\n",
    "[*Tomas Mikolov et al.*](https://arxiv.org/pdf/1301.3781v3.pdf)\n",
    "\n",
    "* __CBOW__ model predict missing word (focus word) using context (surrounding words).\n",
    "* __skip gram__ model is reverse to _CBOW_. It predicts context based on the word in focus.\n",
    "\n",
    "* **Context** is a fixed number of words to the left and right of the word in focus (see picture below). The length of the context is defined by the \"window\" parameter.\n",
    "\n",
    "![context](pics/context.png)\n",
    "\n",
    "Two models comparision\n",
    "\n",
    "![architecture](pics/architecture.png)\n",
    "\n",
    "\n",
    "### Skip_gram\n",
    "\n",
    "Consider a corpus with a sequence of words $ w_1, w_2, .., w_T $.\n",
    "\n",
    "Objective function (we would like to maximize it) for _skip gram_ is defined as follow:\n",
    "\n",
    "\n",
    "$$ AverageLogProbability = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\leqslant j\\leqslant c, j \\neq 0} log\\ p (w_{t+j} | w_t) $$\n",
    "\n",
    "* where $ c $ is a context length.\n",
    "* $w_t$ -- focus word\n",
    "\n",
    "The basic formulation for probability $ p (w_{t+j} | w_t) $ is calculated using __Softmax__ -\n",
    "\n",
    "$$ p (w_h | w_i) = \\frac{exp(s(v_i, v_h))}{ \\sum^{W}_{w=1}  exp(s(v_{w}, v_{i} )) } $$\n",
    "\n",
    "where\n",
    "* $w_i$ -- input focus word\n",
    "* $w_h$ -- hypothetically context word for a given focus word $w_i$\n",
    "* $v_i$ and $v_h$ input-word and hypothesis-word vector representations (for $w_i$, $w_h$)\n",
    "* $s(v_i, v_h) = v^{T} _{h} \\cdot v_{i}$\n",
    "* $W$ is the number of words in vocabulary\n",
    "\n",
    "___\n",
    "\n",
    "### CBOW\n",
    "\n",
    "Predict word using context.\n",
    "\n",
    "$$ E = -log\\ p(w_h\\ |\\ w_{1},\\ w_{2},\\ \\dots,\\ w_{c}) $$\n",
    "\n",
    "\n",
    "The **probability** is the same as in the *skip gram* model, but now $v_i$ is a sum of context-word vectors.\n",
    "\n",
    "$$ p(w_h\\ |\\ w_{1},\\ w_{2},\\ \\dots,\\ w_{c})  = \\frac{exp(s(v_i, v_h))}{\\sum^{W}_{w=1}  exp(s(v_{w}, v_{i}))} $$\n",
    "\n",
    "\n",
    "* $\\ w_{1},\\ w_{2},\\ \\dots,\\ w_{c}$ -- input context words\n",
    "* $w_h$ -- hypothetically focus word for a given context words\n",
    "* $ v_i = \\sum^{c}_{k=1} w_{k}$\n",
    "* $ v_h$ = vector of hypothesis word\n",
    "* $s(v_i, v_h) = v^{T} _{h} \\cdot v_{i}$\n",
    "* $W$ is the number of words in vocabulary\n",
    "\n",
    "___\n",
    "\n",
    "Lets implement __`Skip-gram`__ using tensorflow framework.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 2: preparing the data, building a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you prepare the data for the word2ve neural network by tokenizing it, building a dictionary and encoding it with corresponding identifiers. \n",
    "\n",
    "You may want to use the functions you have written during Seminar 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use this to prepare the 10 times larger file\n",
    "#!perl wikifil.pl enwik9>text9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the texts from csv file and convert them into a single list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text8') as infile:\n",
    "    all_texts = infile.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "# use any tokenizer you deem necessary\n",
    "tokens =  nltk.RegexpTokenizer('\\w+').tokenize(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(tokens, list)\n",
    "assert isinstance(tokens[0], str)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a dictionary {token: token_id} of 50000 most frequent tokens.\n",
    "\n",
    "After that, make an inverse dictionary {token_id: token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(tokens, max_size=20000):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary of at most max_size words from the supplied list of lists of tokens.\n",
    "    If a word embedding model is provided, adds only the words present in the model vocabulary.\n",
    "    \"\"\"\n",
    "    # your code goes here\n",
    "    vocabulary = {}\n",
    "    reserved_symbols = [\"NULL\", \"UNKN\"]\n",
    "    \n",
    "    counter = collections.Counter(tokens)\n",
    "    \n",
    "    freq_toks = counter.most_common(max_size-len(reserved_symbols))\n",
    "\n",
    "    voc_words = [k[0] for k in freq_toks]\n",
    "\n",
    "    for i, reserved in enumerate(reserved_symbols):\n",
    "        vocabulary[reserved] = i\n",
    "\n",
    "    for i, k in enumerate(voc_words):\n",
    "        vocabulary[k] = i+len(reserved_symbols)\n",
    "\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = build_vocabulary(tokens, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary['one']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "reverse_dictionary = {val: keey for keey, val in dictionary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_dictionary[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "assert len(dictionary) == len(reverse_dictionary) == 50000\n",
    "assert sorted(dictionary.keys()) == sorted(reverse_dictionary.values())\n",
    "assert sorted(reverse_dictionary.keys()) == sorted(dictionary.values())\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the tokens into a list of their identifiers from your 'dictionary'. Replace the Out Of Vocabulary [OOV] tokens with 'UNKN' identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokens, token_to_id):\n",
    "    # your code goes here\n",
    "    encoded_tokens = [token_to_id.get(token, token_to_id['UNKN']) \n",
    "     for token in tokens] # get-- ete chka, veradarcni UNKN\n",
    "        \n",
    "    return encoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = encode(tokens, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample encoded data [5236, 3085, 13, 7, 196]\n",
      "Sample decoded data ['anarchism', 'originated', 'as', 'a', 'term']\n"
     ]
    }
   ],
   "source": [
    "print('Sample encoded data', data[:5])\n",
    "print('Sample decoded data', [reverse_dictionary[t] for t in data[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "for t, tid in zip(tokens, data):\n",
    "    assert ((reverse_dictionary[tid] == t) or (tid==dictionary['UNKN']))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are done with data preparation, now train the word2vec model. You don't need to change anything in this section.\n",
    "\n",
    "Read the comments to get a better understanding of what is happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first']\n",
      "\n",
      "with num_skips = 2 and skip_window = 1:\n",
      "    batch: ['originated', 'originated', 'as', 'as', 'a', 'a', 'term', 'term']\n",
      "    labels: ['as', 'anarchism', 'a', 'originated', 'term', 'as', 'a', 'of']\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    # Initilzing a buffer to sample from\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Moving over the the corpus to the next buffer while we get a whole batch\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # set target somewehre, then find the right ones\n",
    "        targets_to_avoid = [ skip_window ]  #don't use the center word as a target\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
    "# Let's test how this works\n",
    "for num_skips, skip_window in [(2, 1)]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow word2vec computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/igel/.local/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-31-966351880690>:48: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "skip_window = 1 # How many words to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a label.\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. \n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "    # Input data.\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "  \n",
    "    # Variables.\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "                                   labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "\n",
    "    # Optimizer.\n",
    "    # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
    "    # This is because the embeddings are defined as a variable quantity and the\n",
    "    # optimizer's `minimize` method will by default modify all variable quantities \n",
    "    # that contribute to the tensor it is passed.\n",
    "    # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
    "    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "  \n",
    "    # Compute the similarity between minibatch examples and all embeddings.\n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 100000: 3.332023\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to which: that, this, usually, what, however, but, typically, cursing,\n",
      "Nearest to would: will, could, can, might, may, should, must, did,\n",
      "Nearest to who: he, she, actually, ever, which, gloss, still, that,\n",
      "Nearest to can: could, may, will, would, should, must, might, cannot,\n",
      "Nearest to their: its, his, her, your, our, the, my, some,\n",
      "Nearest to to: will, must, should, would, could, veracruz, delaunay, otherwise,\n",
      "Nearest to has: had, have, is, since, was, having, makes, gunships,\n",
      "Nearest to is: was, has, are, became, becomes, be, seems, lies,\n",
      "Nearest to zero: four, eight, five, seven, six, nine, three, two,\n",
      "Nearest to at: near, in, on, individualists, counselor, under, during, from,\n",
      "Nearest to after: before, during, when, while, until, despite, without, antonius,\n",
      "Nearest to called: named, known, barra, woodruff, referred, used, considered, described,\n",
      "Nearest to time: lawns, year, way, period, correspondance, alioth, decade, prohibitions,\n",
      "Nearest to are: were, is, have, be, timbres, contain, although, include,\n",
      "Nearest to some: many, several, these, any, certain, various, both, all,\n",
      "Nearest to one: two, four, seven, three, eight, five, nine, six,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "lh = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_data, batch_labels = generate_batch(\n",
    "          batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += l\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000\n",
    "            display.clear_output(wait=True)\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step %d: %f' % (step, average_loss))\n",
    "            lh.append(average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "            plt.figure(figsize=(14, 10))\n",
    "\n",
    "            plt.title(\"Training loss, step {}/{}\".format(step, num_steps))\n",
    "            plt.xlabel(\"#step\")\n",
    "            plt.ylabel(\"loss\")\n",
    "            plt.plot(lh, 'b')\n",
    "            plt.show()\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8 # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log = '%s %s,' % (log, close_word)\n",
    "                print(log)\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the word vectors and dictionaries for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vec_file(final_emb_mtx, vocab_size, vec_size,filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(str(vocab_size)+' '+str(vec_size) + '\\n')\n",
    "        for n in range(vocab_size):\n",
    "            s = ' '.join([reverse_dictionary[n]] + [str(num) for num in final_emb_mtx[n]])\n",
    "            f.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_vec_file(final_embeddings, vocab_size=vocabulary_size, vec_size=embedding_size, filename='simple_cbow.w2v')\n",
    "pickle.dump([dictionary, reverse_dictionary], open('dict_rdict.pkl',\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T_SNE projection of word vectors into a 2-dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'topical_words.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-610489e72176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'topical_words.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'colors'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vegetables'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'numbers'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'professions'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'GoldenRod'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'topical_words.json'"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "dd = json.load(open('topical_words.json'))\n",
    "colors = {'colors':'r', 'vegetables':'g', 'numbers':'b', 'professions': 'GoldenRod'}\n",
    "wd = {}\n",
    "for k in dd:\n",
    "    for w in dd[k]:\n",
    "        if w in dictionary:\n",
    "            wd[w] = {'emb': final_embeddings[dictionary[w]], 'label': k, 'color': colors[k]}\n",
    "            \n",
    "M = np.array([wd[k]['emb'] for k in wd])\n",
    "labels = [wd[k]['label'] for k in wd]\n",
    "colors = [wd[k]['color'] for k in wd]\n",
    "\n",
    "P = squareform(pdist(M, metric='cosine'))\n",
    "tsne2 = TSNE(n_components=2, random_state=34, metric='precomputed', n_iter=9001)\n",
    "Y = tsne2.fit_transform(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-4c06c56c8a0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'color'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     plt.annotate(k, xy=(Y[i][0], Y[i][1]), xytext=(0, 0), textcoords='offset points', \n\u001b[1;32m      5\u001b[0m                  color=wd[k]['color'], fontsize=12)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wd' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1440 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for i, k in enumerate(wd):\n",
    "    plt.scatter([Y[i][0]], [Y[i][1]], color = wd[k]['color'], label=wd[k]['label'])\n",
    "    plt.annotate(k, xy=(Y[i][0], Y[i][1]), xytext=(0, 0), textcoords='offset points', \n",
    "                 color=wd[k]['color'], fontsize=12)\n",
    "\n",
    "\n",
    "plt.title('T-SNE on word2vec representations, 4 topics', fontsize=16)        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
