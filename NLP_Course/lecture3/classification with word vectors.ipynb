{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_data = pd.read_csv(\"./train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TO_USE = 250000\n",
    "\n",
    "texts = sentiment_data['message'].tolist()[:DATA_TO_USE]\n",
    "labels = np.array(sentiment_data['sentiment'])[:DATA_TO_USE]\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, random_state=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: word vectors meet bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you use your newly trained word vectors and a simple Bag of Words models to approach the sentiment analysis task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will use a convinient wrapper for our word2vec model provided by gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load_word2vec_format(\"/home/igel/Downloads/simple_cbow.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.02990426, -0.15916565, -0.11447234, -0.05986521, -0.12276778,\n",
       "       -0.16413523, -0.11490295,  0.02804964, -0.00436324,  0.01494077,\n",
       "       -0.01524218,  0.1097708 , -0.06026166, -0.04513515, -0.00505505,\n",
       "        0.09611265,  0.07788374,  0.08366123, -0.01655415,  0.12933229,\n",
       "       -0.03674443, -0.19012986,  0.05885392,  0.06013624,  0.0801625 ,\n",
       "        0.03977867,  0.00211081,  0.06386852, -0.03472841,  0.15342017,\n",
       "        0.05833793, -0.05921539, -0.05952489, -0.06266541, -0.08161964,\n",
       "        0.12462132, -0.00591985, -0.15344228,  0.12336656,  0.03647648,\n",
       "        0.05483676,  0.06484858, -0.07936434, -0.09052481,  0.03851042,\n",
       "        0.02385422, -0.05429959,  0.07028124,  0.04387809, -0.11558681,\n",
       "        0.05841981, -0.12859698, -0.0077426 ,  0.19667299,  0.02155624,\n",
       "       -0.02183611,  0.10042851, -0.0544624 , -0.00891632,  0.2033658 ,\n",
       "       -0.013372  ,  0.06375191, -0.11004724, -0.15569478, -0.08661045,\n",
       "        0.00989923, -0.08449263, -0.1268454 , -0.09556618,  0.05708125,\n",
       "        0.01733651, -0.05915393,  0.05831371, -0.13105504, -0.15154782,\n",
       "       -0.02846842,  0.04055947, -0.00177838, -0.16879214, -0.08253083,\n",
       "        0.04791364, -0.10665137,  0.01209988,  0.07055962, -0.06417862,\n",
       "        0.0064663 ,  0.03835146, -0.12100882, -0.06466809, -0.05468452,\n",
       "       -0.07967035, -0.13575897,  0.0668559 ,  0.03564537,  0.03588279,\n",
       "       -0.0124353 , -0.03060941, -0.13109188,  0.03905683, -0.00744886,\n",
       "        0.0109439 , -0.04480705,  0.02463673,  0.00499177,  0.20794976,\n",
       "        0.11404622,  0.07627168,  0.01447007,  0.09103655, -0.1586459 ,\n",
       "       -0.01299577,  0.01739187, -0.1198793 ,  0.05925678, -0.14033644,\n",
       "        0.05444143, -0.08498427, -0.0649891 ,  0.0538366 ,  0.09758718,\n",
       "        0.141524  ,  0.0005939 , -0.04150689, -0.03933578, -0.07242383,\n",
       "        0.1208585 , -0.10929849, -0.1977357 ], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can get the vector for a word in a simple way\n",
    "print(len(w2v_model['word']))\n",
    "w2v_model['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.Word2VecKeyedVectors"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igel/.local/lib/python3.5/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('another', 0.4291095733642578),\n",
       " ('the', 0.38493815064430237),\n",
       " ('any', 0.37689319252967834),\n",
       " ('coffey', 0.34323281049728394),\n",
       " ('jacqueline', 0.34044143557548523),\n",
       " ('kneeling', 0.3361562192440033),\n",
       " ('mackay', 0.32636597752571106),\n",
       " ('stresemann', 0.31539812684059143),\n",
       " ('histone', 0.31503355503082275),\n",
       " ('monaghan', 0.3137550354003906)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can easily query the model for word most similar to a give word \n",
    "w2v_model.most_similar('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you learn how to encode sentences with word2vec using a bag of words approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a tokenizer that you will use throughout the exercise\n",
    "# I would recommend a regexp tokenizer for speed, but it's completely up to you\n",
    "def my_tokenizer(text):\n",
    "    return nltk.regexp_tokenize(text, '\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_encoder(wmodel, tokenizer, text):\n",
    "    \"\"\"\n",
    "    This function encodes text into a vector.\n",
    "    \n",
    "    First, it tokenizes input text using the provided tokenizer function.\n",
    "    Then it uses the provided word2vec model to get the vectors corresponding to text's tokens.\n",
    "    Finally, it computes an average of all token's vectors and returns it.\n",
    "    \n",
    "    If the function failed to find and encode any words, it should at least return a vector of zeros.\n",
    "    \"\"\"\n",
    "    zero_vector = np.zeros(w2v_model.vector_size)\n",
    "    # your code goes here\n",
    "#     sent_vector = [wmodel.get(token, zero_vector) for token in tokenized_text]\n",
    "    tokens = tokenizer(text)\n",
    "    word_vectors = [wmodel[t] for t in tokens if t in wmodel]\n",
    "    if len(word_vectors):\n",
    "        sent_vector = np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        sent_vector = np.zeros(w2v_model.vector_size)\n",
    "    return sent_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use your new encoder to encode both train_texts and test_texts into matrices.\n",
    "\n",
    "The number of rows in a matrix should be equal to the number of texts encoded.\n",
    "\n",
    "The number of columns should be equal to the word2vec space dimansionality (currently = 128)\n",
    "\n",
    "Just write a little loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence_with_bow(sentences, model, tokenizer):\n",
    "    return np.array([bow_encoder(model, tokenizer, text) for text in tqdm(sentences)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187500/187500 [00:11<00:00, 16523.28it/s]\n",
      "100%|██████████| 62500/62500 [00:03<00:00, 17374.45it/s]\n"
     ]
    }
   ],
   "source": [
    "train_encoded =  encode_sentence_with_bow(train_texts, w2v_model, my_tokenizer)\n",
    "test_encoded  =  encode_sentence_with_bow(test_texts, w2v_model, my_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(train_encoded, np.ndarray)\n",
    "assert isinstance(test_encoded, np.ndarray)\n",
    "\n",
    "assert train_encoded.shape[0] == len(train_texts)\n",
    "assert train_encoded.shape[1] == w2v_model.vector_size\n",
    "\n",
    "assert test_encoded.shape[0] == len(test_texts)\n",
    "assert test_encoded.shape[1] == w2v_model.vector_size\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(187500, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.02896139,  0.00537474, -0.01987924,  0.03310338,  0.00644111,\n",
       "        0.06600953,  0.01550328, -0.03566363, -0.02510163, -0.01117753,\n",
       "        0.00322492,  0.04902239,  0.01001703,  0.05570917,  0.03689624,\n",
       "       -0.01485302, -0.02157233, -0.00184187,  0.02044139, -0.0059447 ,\n",
       "       -0.01276293, -0.07878712,  0.05575456, -0.0461246 , -0.00306651,\n",
       "        0.00723166, -0.01750634,  0.04136015,  0.00466379,  0.0240885 ,\n",
       "       -0.00868364,  0.00425283, -0.03041368,  0.02261088,  0.04232081,\n",
       "       -0.04063541,  0.00036504, -0.02799336, -0.01585136, -0.01942656,\n",
       "        0.00216601,  0.00431208, -0.01391252, -0.00474886,  0.05443996,\n",
       "        0.01660595, -0.03581998,  0.01215116,  0.03361015, -0.03727384,\n",
       "       -0.00317842,  0.00865125, -0.02583007,  0.03629126,  0.04480203,\n",
       "       -0.01947688,  0.05272445, -0.00338176,  0.0287633 , -0.02610079,\n",
       "        0.02351554,  0.04307071, -0.02624363, -0.02532157,  0.03658989,\n",
       "        0.0527278 , -0.01743673, -0.04904271, -0.0402409 , -0.00550947,\n",
       "       -0.01922494, -0.00671998, -0.0101804 ,  0.04076944, -0.01559357,\n",
       "       -0.02317916, -0.02938828, -0.00631467,  0.02857715, -0.00923564,\n",
       "        0.04164547,  0.01786685,  0.00662936, -0.01311591,  0.01127424,\n",
       "        0.03514919, -0.0058687 ,  0.0074058 , -0.01371941, -0.04127634,\n",
       "        0.03189777, -0.00830887, -0.03699375,  0.01141259, -0.01340273,\n",
       "        0.07664905,  0.02106955,  0.002698  ,  0.05050638, -0.04572219,\n",
       "        0.04317769, -0.02279144, -0.05568022, -0.01264595,  0.00787132,\n",
       "        0.01141632, -0.03269797, -0.03157479, -0.03561372, -0.03886346,\n",
       "       -0.0062407 , -0.04895879, -0.00103197, -0.01583979, -0.05252172,\n",
       "        0.02078566, -0.01094007, -0.00164576, -0.01606467,  0.03496647,\n",
       "       -0.0111823 ,  0.0024573 ,  0.00525956, -0.01899202, -0.02183154,\n",
       "       -0.02994526, -0.02566474,  0.00061388])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_encoded.shape)\n",
    "train_encoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.64      0.63     31237\n",
      "          1       0.63      0.62      0.63     31263\n",
      "\n",
      "avg / total       0.63      0.63      0.63     62500\n",
      "\n",
      "AUC = 0.6306270643977973\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(train_encoded, train_labels)\n",
    "preds = clf.predict(test_encoded)\n",
    "\n",
    "print(classification_report(test_labels, preds))\n",
    "print(\"AUC = {}\".format(roc_auc_score(test_labels, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not too impressive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you attempt to improve your encoder by filtering out stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_encoder_with_stopwords(wmodel, tokenizer, stopwords, text):\n",
    "    \"\"\"\n",
    "    This function encodes text into a vector.\n",
    "    \n",
    "    First, it tokenizes input text using the provided tokenizer function.\n",
    "    Then it removes any stopwords from the list of tokens.\n",
    "    Then it uses the provided word2vec model to get the vectors corresponding to text's tokens.\n",
    "    Finally, it computes an average of all token's vectors and returns it.\n",
    "    \n",
    "    If the function failed to find and encode any words, it should at least return a vector of zeros.\n",
    "    \"\"\"\n",
    "    zero_vector = np.zeros(w2v_model.vector_size)\n",
    "    # your code goes here\n",
    "    tokens_ = tokenizer(text)\n",
    "    tokens  = [token for token in tokens_ if (token in wmodel and token.lower() not in stopwords)]\n",
    "    word_vectors = [wmodel[t] for t in tokens if t in wmodel]\n",
    "    if len(word_vectors):\n",
    "        sent_vector = np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        sent_vector = np.zeros(w2v_model.vector_size)\n",
    " \n",
    "    return sent_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence_with_bow_stop(sentences, model, tokenizer):\n",
    "    return np.array([bow_encoder_with_stopwords(model, tokenizer, stops, text) for text in tqdm(sentences)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187500/187500 [00:09<00:00, 19147.96it/s]\n",
      "100%|██████████| 62500/62500 [00:03<00:00, 19150.76it/s]\n"
     ]
    }
   ],
   "source": [
    "train_encoded_ =  encode_sentence_with_bow_stop(train_texts, w2v_model, my_tokenizer)\n",
    "test_encoded_  =  encode_sentence_with_bow_stop(test_texts, w2v_model, my_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.62      0.61     31237\n",
      "          1       0.61      0.61      0.61     31263\n",
      "\n",
      "avg / total       0.61      0.61      0.61     62500\n",
      "\n",
      "AUC = 0.6110897364727454\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(train_encoded_, train_labels)\n",
    "preds = clf.predict(test_encoded_)\n",
    "\n",
    "print(classification_report(test_labels, preds))\n",
    "print(\"AUC = {}\".format(roc_auc_score(test_labels, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.611088"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looks like the BoW model is not too good for the job!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![architecture](pics/we_need_to_go_deeper.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing: Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is a cool library built on top of the computational backend provided by Tensorflow. It provides a layer of abstraction between you and complicated tensor algebra, allowing for rapid prototyping of deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.3: Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start crunching word vectors with convolutional neural networks, we need to prepare our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vocabulary we created earlier\n",
    "voc, rvoc = pickle.load(open(\"./dict_rdict.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rvoc[179]\n",
    "voc['set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to use the whole dataset this time around\n",
    "texts = sentiment_data['message'].tolist()\n",
    "labels = np.array(sentiment_data['sentiment'])\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, random_state=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the function that turns tokens into their ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint: you may want to use the function you've built during seminar 2\n",
    "MAX_LEN = 32\n",
    "\n",
    "def vectorize_tokens(sentence, tokenizer, token_to_id, max_len):\n",
    "    \"\"\"\n",
    "    Preprocesses a sentence into list of tokens using the provided tokenizer\n",
    "    Then converts it into a list of token ids using the supplied 'token_to_id' dictionary.\n",
    "    Pads resulting list with NULL identifiers up to max_len length. \n",
    "    \"\"\"\n",
    "    # your code goes here\n",
    "    # STEP 1: convert sentence to a list of tokens\n",
    "    tokens = tokenizer(sentence)\n",
    "    # STEP 2: replace tokens with their identifiers from the vocabulary\n",
    "    # If the token is not present in the vocabulary, replace it with UNKN identifier\n",
    "    ids = [token_to_id.get(token, token_to_id['UNKN']) for token in tokens]    \n",
    "\n",
    "    # STEP 3: pad the sequence id's with NULL identifiers until so that it's length is equal to max_len\n",
    "#     while len(ids) < max_len:\n",
    "#         ids.append(token_to_id['NULL'])\n",
    "\n",
    "    if len(ids) > max_len:\n",
    "        ids = ids[:max_len]\n",
    "    elif len(ids) < max_len:\n",
    "        miss_count = max_len - len(ids)\n",
    "        ids.extend([token_to_id['NULL']]*miss_count)\n",
    "        \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 228,\n",
       " 1301,\n",
       " 12,\n",
       " 21,\n",
       " 1,\n",
       " 177,\n",
       " 39,\n",
       " 316,\n",
       " 368,\n",
       " 1315,\n",
       " 1,\n",
       " 2,\n",
       " 44,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_tokens('the main difference is that KeyedVectors do not support further training. On the other',\n",
    "                 my_tokenizer, voc, MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply the vectorization function to every sentence from train and test datasets. In the end you should end up with a matrix of shape [len(data), MAX_LEN].\n",
    "\n",
    "Just write a little loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentences(sentences, tokenizer, token_to_id, max_len):\n",
    "    sentence_ids = []\n",
    "    \n",
    "    # your code goes here\n",
    "    sentence_ids = [vectorize_tokens(sentence, tokenizer, token_to_id, max_len) for sentence in tqdm(sentences)]\n",
    "        \n",
    "    return np.array(sentence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937500/937500 [00:13<00:00, 70878.23it/s]\n",
      "100%|██████████| 312500/312500 [00:04<00:00, 70312.17it/s]\n"
     ]
    }
   ],
   "source": [
    "train_vectorized = vectorize_sentences(train_texts, my_tokenizer, voc, MAX_LEN)\n",
    "test_vectorized = vectorize_sentences(test_texts, my_tokenizer, voc, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(937500, 32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(train_vectorized, np.ndarray)\n",
    "assert isinstance(test_vectorized, np.ndarray)\n",
    "\n",
    "assert train_vectorized.shape == (len(train_vectorized), MAX_LEN)\n",
    "assert test_vectorized.shape == (len(test_vectorized), MAX_LEN)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Building a deep NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igel/.local/lib/python3.5/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "embeddings_matrix = w2v_model.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 128)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras Input layer is basically the same thing as tf.placeholder\n",
    "# it defines a node where the network will be expecting to recieve input data\n",
    "input_layer = keras.layers.Input(shape=(MAX_LEN,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras Embedding layer is a container for dense vectors\n",
    "# it recieves a list of token identifiers of shape [MAX_LEN] \n",
    "# and turns it into a matrix of shape [MAX_LEN, EMBEDDING_DIM]\n",
    "\n",
    "embedding_layer = keras.layers.Embedding(embeddings_matrix.shape[0], embeddings_matrix.shape[1], \n",
    "                                         input_length=MAX_LEN, weights=[embeddings_matrix],\n",
    "                                         trainable=False)(input_layer)\n",
    "# notice how the input_layer is plugged into the embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras Convolutional layer implements a set of learnable filters\n",
    "# that extract local patterns from input data\n",
    "convolution_layer = keras.layers.Convolution1D(128, 3)(embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras GlobalMaxPooling layer applies a max filter to the input feature representation\n",
    "# only the strongest responses from the previous layer are kept, everything else is discarded\n",
    "subsampling_layer = keras.layers.GlobalMaxPooling1D()(convolution_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras Linear layers apply a simple linear transformation to input data, \n",
    "# which is optionally followed by a non-linear activation function\n",
    "# very useful for building Multi-Layer Perceptrons\n",
    "linear_layer_1 = keras.layers.Dense(64, activation='relu')(subsampling_layer)\n",
    "linear_layer_2 = keras.layers.Dense(1, activation='sigmoid')(linear_layer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this compiles the computational graph we've just created, applies a loss function\n",
    "# and pre-computes the gradients for back propagation\n",
    "\n",
    "deep_model = keras.models.Model(inputs=[input_layer], outputs=[linear_layer_2])\n",
    "deep_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 32, 128)           6400000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 30, 128)           49280     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 6,457,601\n",
      "Trainable params: 57,601\n",
      "Non-trainable params: 6,400,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "deep_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 937500 samples, validate on 312500 samples\n",
      "Epoch 1/3\n",
      "937500/937500 [==============================] - 175s 187us/step - loss: 0.5193 - acc: 0.7401 - val_loss: 0.4960 - val_acc: 0.7561\n",
      "Epoch 2/3\n",
      "937500/937500 [==============================] - 178s 190us/step - loss: 0.4842 - acc: 0.7649 - val_loss: 0.4984 - val_acc: 0.7575\n",
      "Epoch 3/3\n",
      "937500/937500 [==============================] - 180s 192us/step - loss: 0.4713 - acc: 0.7735 - val_loss: 0.4793 - val_acc: 0.7679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2e8a292908>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_model.fit(x=train_vectorized, y=train_labels, batch_size=64, epochs=3, \n",
    "               validation_data=[test_vectorized, test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = deep_model.predict(test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.8503456596197503\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC = {}\".format(roc_auc_score(test_labels, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_model.save_weights(\"nn_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thats more like it! Keep in mind that we only trained a tiny model because of the limitations of CPU computing power. Using a deeper model with more trainable filters in the Convolution layer would likely result in even stronger predictive power. Stay tuned! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
